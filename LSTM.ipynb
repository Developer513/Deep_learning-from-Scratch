{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"mount_file_id":"15nfU9XHZY_dS8w9yzL5HYx1WXvNu4QBK","authorship_tag":"ABX9TyMGToYivGIWOBNcCAadEUwj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"DXYQj4-P4IlF","colab_type":"code","colab":{}},"source":["#LSTM 이란 일종의 RNN 과 유사하게 작동하지만 게이팅 메커니즘이 이를 차별화 시킨다 이 기능은 RNN의 단기 메모리 문제를 해결합니다. \n","# Long Short Term Memory\n","# 바로이전에의 데이터 결과, 몇단계전 데이터결과 값을 넘겨줄지 말지를 필터(게이트)가 결정한다. \n","#Vanila 단순RNN"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ANdjPHWp4MkD","colab_type":"code","colab":{}},"source":["import bz2\n","from collections import Counter\n","import re\n","import nltk\n","import numpy as np\n","nltk.download('punkt')\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","train_file = bz2.BZ2File('/content/gdrive/My Drive/Test/amazone_reviews/train.ft.txt.bz2')\n","test_file = bz2.BZ2File('/content/gdrive/My Drive/Test/amazone_reviews/test.ft.txt.bz2')\n","train_file = train_file.readlines()\n","#readlines()로 파일을 읽으면 한 줄, 한 줄이 각각 리스트의 원소로 들어간다.\n","#파일 전체가 lines라는 리스트에 담기는 모양. 그 다음엔 sys 모듈을 이용해서 제어가능 .\n","test_file = test_file.readlines()\n","\n","print(\"Number of training reviews: \" + str(len(train_file)))\n","# 훈련데이터 3600000만줄 \n","print(\"Number of test reviews: \" + str(len(test_file)))\n","# 테스트 데이터 사십만줄\n","\n","num_train = 800000\n","num_test = 200000\n","# reference environment\n","train_file = [x.decode('utf-8') for x in train_file[:num_train]]\n","test_file = [x.decode('utf-8') for x in test_file[:num_test]]\n","\n","print(train_file[0]) # 훈련데이터 첫줄 출력 \n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VDACUQNVBjwG","colab_type":"code","colab":{}},"source":["train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file]\n","train_sentences = [x.split(' ',1)[1][:-1].lower() for x in train_file]\n","\n","test_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file]\n","test_sentences = [x.split(' ',1)[1][:-1].lower() for x in test_file]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U__Jq5XrCLYf","colab_type":"code","colab":{}},"source":["for i in range(len(train_sentences)):\n","  train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n","  # re.sub('패턴','바꿀문자열','문자열','바꿀횟수') 정규표현식 문자열 변환 \n","\n","for i in range(len(test_sentences)):\n","  test_sentences[i] = re.sub('\\d','0',test_sentences[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Fyb2yT8C8Ja","colab_type":"code","colab":{}},"source":["for i in range(len(train_sentences)):\n","  if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n","    train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n","\n","for i in range(len(test_sentences)):\n","  if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n","    test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])\n","\n","del train_file, test_file\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GJ86k0EREiav","colab_type":"code","colab":{}},"source":["words = Counter()\n","for i,sentence in enumerate(train_sentences):\n","    train_sentences[i] = []\n","    for word in nltk.word_tokenize(sentence):\n","      words.update([word.lower()])\n","      train_sentences[i].append(word)\n","    if i%20000 == 0:\n","      print(str((i*100)/num_train)+\"% done\")\n","print(\"100% done\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QQfnUav9FoZj","colab_type":"code","colab":{}},"source":["words = {k:v for k,v in words.items() if v>1}\n","words = sorted(words, key=words.get, reverse = True)\n","\n","words = ['_PAD', '_UNK'] + words\n","word2idx = {o:i for i,o in enumerate(words)}\n","idx2word = {i:o for i,o in enumerate(words)}\n","\n","for i, sentence in enumerate(train_sentences):\n","  train_sentences[i] = [word2idx[word] if word in word2idx else word2idx['_UNK'] for word in sentence]\n","\n","for i, sentence in enumerate(test_sentences):\n","  test_sentences[i] = [word2idx[word.lower()] if word.lower() in word2idx else word2idx['_UNK'] for word in sentence]\n","\n","def pad_input(sentences, seq_len):\n","  features = np.zeros((len(sentences), seq_len), dtype = int)\n","  for ii, review in enumerate(sentences):\n","    if len(review) != 0:\n","      features[ii, -len(review):] = np.array(review)[:seq_len]\n","  return features"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pXoM_4QdHiUl","colab_type":"code","colab":{}},"source":["seq_len = 200\n","train_sentences = pad_input(train_sentences, seq_len)\n","test_sentences = pad_input(test_sentences, seq_len)\n","\n","train_labels = np.array(train_labels)\n","test_labels = np.array(test_labels)\n","\n","test_sentences[0]\n","\n","split_frac = 0.5\n","split_id = int(split_frac * len(test_sentences))\n","val_sentences, test_sentences = test_sentences[:split_id], test_sentences[split_id:]\n","val_labels, test_labels = test_labels[:split_id], test_labels[split_id:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N8NLsqdSIb15","colab_type":"code","colab":{}},"source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","train_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\n","val_data = TensorDataset(torch.from_numpy(val_sentences), torch.from_numpy(val_labels))\n","test_data = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n","\n","batch_size = 400\n","\n","train_loader = DataLoader(train_data, shuffle = True, batch_size = batch_size)\n","val_loader = DataLoader(val_data, shuffle = True, batch_size = batch_size)\n","test_loader = DataLoader(test_data, shuffle = True, batch_size = batch_size)\n","is_cuda = torch.cuda.is_available()\n","if is_cuda:\n","  device = torch.device(\"cuda\")\n","  print(\"GPU is available\")\n","else:\n","  device = torch.device(\"cpu\")\n","  print(\"GPU not available, CPU used\")\n","\n","dataiter = iter(train_loader)\n","sample_x, sample_y = dataiter.next()\n","print(sample_x.shape, sample_y.shape)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_lgwSQOVKbsU","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","class SentimentNet(nn.Module):\n","  def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob = 0.5):\n","    super(SentimentNet, self).__init__()\n","    self.output_size = output_size\n","    self.n_layers = n_layers\n","    self.hidden_dim = hidden_dim\n","    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","    self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout = drop_prob, batch_first = True)\n","    self.dropout = nn.Dropout(0.2)\n","    self.fc = nn.Linear(hidden_dim, output_size)\n","    self.sigmoid = nn.Sigmoid()\n","  def forward(self, x, hidden):\n","    batch_size = x.size(0)\n","    x = x.long()\n","    embeds = self.embedding(x)\n","    lstm_out, hidden = self.lstm(embeds,hidden)\n","    lstm_out = lstm_out.contiguous().view(-1,self.hidden_dim)\n","\n","    out = self.dropout(lstm_out)\n","    out = self.fc(out)\n","    out = self.sigmoid(out)\n","    out = out.view(batch_size, -1)\n","    out = out[:,-1]\n","    return out, hidden\n","  def init_hidden(self, batch_size):\n","    weight = next(self.parameters()).data\n","    hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),weight.new(self.n_layers, batch_size,self.hidden_dim).zero_().to(device))\n","    return hidden\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PQiFrc2pMrZj","colab_type":"code","colab":{}},"source":["vocab_size = len(word2idx) +1\n","output_size = 1\n","embedding_dim = 400\n","hidden_dim = 512\n","n_layers = 2\n","\n","model = SentimentNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n","model.to(device)\n","print(model)\n","lr = 0.005\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n","\n","epochs = 2\n","counter = 0\n","print_every = 1000\n","clip = 5\n","valid_loss_min = np.Inf\n","\n","model.train()\n","for i in range(epochs):\n","  h = model.init_hidden(batch_size)\n","  for inputs, labels in train_loader:\n","    counter +=1\n","    h = tuple([e.data for e in h])\n","    inputs, labels = inputs.to(device), labels.to(device)\n","    model.zero_grad()\n","    output, h = model(inputs, h)\n","    loss = criterion(output.squeeze(), labels.float())\n","    loss.backward()\n","    nn.utils.clip_grad_norm_(model.parameters(), clip)\n","    optimizer.step()\n","    \n","    if counter%print_every == 0:\n","      val_h = model.init_hidden(batch_size)\n","      val_losses = []\n","      model.eval()\n","      for inp, lab in val_loader:\n","        val_h = tuple([each.data for each in val_h])\n","        inp, lab = inp.to(device), lab.to(device)\n","        out, val_h = model(inp,val_h)\n","        val_loss = criterion(out.squeeze(), lab.float())\n","        val_losses.append(val_loss.item())\n","      \n","      model.train()\n","      print(\"Epoch : {}/{}...\".format(i+1,epochs),\n","            \"Step : {}...\".format(counter),\n","            \"Loss: {:.6f}...\".format(np.mean(val_losses)),\n","            \"Val Loss: {:.6f}\" .format(np.mean(val_losses)))\n","      if np.mean(val_losses) <= valid_loss_min:\n","        torch.save(model.state_dict(),'/content.gdrive/My Drive/Test/amazone_reviews/state_dict.pt')\n","        print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model...'.format(valid_loss_min,np.mean(val_losses)))\n","        valid_loss_min = np.mean(val_losses)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c_oj05-uQt9U","colab_type":"code","colab":{}},"source":["model.load_state_dict(torch.load('/content.gdrive/My Drive/Test/amazone_reviews/state_dict.pt'))\n","\n","test_losses = []\n","num_correct = 0\n","h = model.init_hidden(batch_size)\n","model.eval()\n","for inputs, labels in test_loader:\n","  h = tuple([each.data for each in h])\n","  inputs, labels = inputs.to(device), labels.to(device)\n","  output, h = model(inputs, h)\n","  test_loss = criterion(output.squeeze(), labels.float())\n","  test+losses.append(test_loss.item())\n","  pred = torch.round(output.squeeze())\n","  correct_tensor = pred.eq(labels.float().vies_as(pred))\n","  corrent = np.squeeze(correct_tensor.cpu().numpy())\n","  num_correct += np.num(correct)\n","print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n","test_acc = num_correct/len(test_loader.dataset)\n","print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mIYV4vLZEri4","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}