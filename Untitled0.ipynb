{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"mount_file_id":"1mxwmZnWlHRG4c6By9cWVneKeCk7dNAY7","authorship_tag":"ABX9TyN0NRxKKvpzw72crTTSeFBl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"roB2jSjaFtt4","colab_type":"code","colab":{}},"source":["from __future__ import unicode_literals, print_function, division\n","from io import open\n","import unicodedata\n","import string\n","import re\n","import random\n","\n","import torch \n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","SOS_token = 0\n","EOS_token = 1\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N7Y_Jvv-Gw2_","colab_type":"code","colab":{}},"source":["class  Lang:\n","  def __init__(self, name):\n","    self.name = name\n","    self.word2index = {}\n","    self.word2count = {}\n","    self.index2word = {0: \"SOS\", 1: \"EOS\"}\n","    self.n_words = 2 \n","  def addSentence(self, sentence):\n","    for word in sentence.split(' '):\n","      self.addWord(word)\n","  def addWord(self, word):\n","    if word not in self.word2index:\n","      self.word2index[word] = self.n_words\n","      self.word2count[word] = 1\n","      self.index2word[self.n_words] = word\n","      self.n_words += 1\n","    else:\n","      self.word2count[word] += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4kpWVZmHHzX4","colab_type":"code","colab":{}},"source":["def unicodeToAscii(s):\n","  return ' '.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZbzFAftmIQsH","colab_type":"code","colab":{}},"source":["def normalizeString(s):\n","  s = unicodeToAscii(s.lower().strip())\n","  s = re.sub(r\"([.!?])\", r\" \\1\",s)\n","  s = re.sub(r\"[^a-zA-Z.!?]+\",r\" \",s)\n","  return s\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sTiMrUKDI7q1","colab_type":"code","colab":{}},"source":["def readLangs(lang1, lang2, reverse = False):\n","  print(\"Reading lines...\")\n","  lines = open('/content/gdrive/My Drive/Test/data/%s-%s.txt' % (lang1, lang2), encoding = 'utf-8').read().strip().split('\\n')\n","  pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n","  if reverse: \n","    pairs = [list(reversed(p)) for p in pairs]\n","    input_lang = Lang(lang2)\n","    output_lang = Lang(lang1)\n","  else:\n","    input_lang = Lang(lang1)\n","    output_lang = Lang(lang2)\n","  return input_lang, output_lang, pairs\n","\n","MAX_LENGTH = 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9WfxOmxtJgVD","colab_type":"code","colab":{}},"source":["eng_prefixes = (\n","    \"i am \", \"i m \",\n","    \"he is\", \"he s \",\n","    \"she is\", \"she s \",\n","    \"you are\", \"you re \",\n","    \"we are\", \"we re \",\n","    \"they are\", \"they re\"\n",")\n","\n","def filterPair(p):\n","  return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH and p[1].startswith(eng_prefixes)\n","\n","def filterPairs(pairs):\n","  return [pair for pair in pairs if filterPair(pair)]\n","\n","def prepareData(lang1, lang2, reverse = False):\n","  input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n","  print(\"Read %s sentence pairs\" % len(pairs))\n","  pairs = filterPairs(pairs)\n","  print('Trimmed to %s sentence pairs ' % len(pairs))\n","  print(\"Counting words...\")\n","  for pair in pairs:\n","    input_lang.addSentence(pair[0])\n","    output_lang.addSentence(pair[1])\n","  print(\"Counted words:\")\n","  print(input_lang.name, input_lang.n_words)\n","  print(output_lang.name, output_lang.n_words)\n","  return input_lang, output_lang, pairs\n","\n","input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n","print(random.choice(pairs))\n","\n","\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-CiHtCIaNtIW","colab_type":"code","colab":{}},"source":["class EncoderRNN(nn.Module):\n","  def __init__(self,input_size, hidden_size):\n","    super(EncoderRNN, self).__init__()\n","    self.hidden_size = hidden_size\n","    self.embedding = nn.Enbedding(input_size, hidden_size)\n","    self.gru = nn.GRU(hidden_size, hidden_size)\n","  def forward(self, input, hidden):\n","    embedded = self.embedding(input).view(1, 1, -1)\n","    output = embedded\n","    output, hidden = dself.gru(output,hidden)\n","    return output, hidden_size\n","  def initHidden(self):\n","    return torch.zeros(1,1, self.hidden_size, device = device)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nhay2YmZOmcs","colab_type":"code","colab":{}},"source":["class DecoderRNN(nn.Module):\n","  def __init__(self, hodden_size, output_size):\n","    super(DecoderRNN, self).__init__()\n","    self.hiddden_size = hiddden_size\n","    self.embedding = nn.Embedding(output_size, hidden_size)\n","    self.gru = nn.GRU(hidden_size, hidden_size)\n","    self.out = nn.Linear(hidden_size, output_size)\n","    self.softmax = nn.LogSoftmax(dim = 1)\n","  def forward(self, input, hidden):\n","    output = self.embedding(input).view(1, 1, -1)\n","    output = F.relu(output)\n","    output, hidden = self.gru(output, hidden)\n","    output = self.softmax(self.out(output[0]))\n","    return output, hidden\n","  def initHidden(self):\n","    return torch.zeros(1,1, self.hidden_size, device = device)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xy8zyW2PP1QF","colab_type":"code","colab":{}},"source":["class AttenDecoderRNN(nn.Module):\n","  def __init__(self, hidden_size, output_size, dropout_p = 0.1, max_length = MAX_LENGTH):\n","    super(AttenDecoderRNN, self).__init__()\n","    self.hidden_size = hidden_size\n","    self.output_size = output_size\n","    self.dropout_p = dropout_p\n","    self.max_length = max_length\n","\n","    self.embedding = Embedding(self.output_size,self.hidden_size)\n","    self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n","    self.attn_combine = nn.Linear(self.hidden_size * 2 , self.hidden_size)\n","    self.dropout = nn.Dropout(self.dropout_p)\n","    self.gru = nn.GRU(self.hidden_size, self.output_size)\n","\n","  def forward(self, input, hidden, encoder_outputs):\n","    embedded = self.embedding(input).view(1,1,-1)\n","    embedded = self.dropout(embedded)\n","    attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hodden[0]), 1)), dom = 1)\n","    attn_applied = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs.unsqueeze(0))\n","\n","    output = torch.cat((embedded[0], attn_applied[0]),1)\n","    output = self.attn_combined(output).unsqueeze(0)\n","    output = F.relu(output)\n","    output,hidden = self.gru(output, hidden)\n","    output = F.log_softmax(self.out(output[0]), dim = 1)\n","    return output, hidden, attn_weights\n","  def initHidden(self):\n","    return torch.zeros(1,1, self.hidden_size, device =device)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BP1omA0HSgIY","colab_type":"code","colab":{}},"source":["def indexesFromSentence(lang, sentence):\n","  indexes = indexesFromSentence(lan, sentence)\n","  indexes.append(EOS_token)\n","  return torch.tensor(indexes, dtype = torch.long, device=device).view(-1,1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wh0keX-UTEUJ","colab_type":"code","colab":{}},"source":["def tensorFromPair(pair):\n","  input_tensor = tensorFromSentence(input_lang, pair[0])\n","  target_tensor = tensorFromSentence(output_lang, pair[1])\n","  return (input_tensor, target_tensor)\n","# ?\n","teacher_forcing_ratio = 0.5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tTyznRKoTQ42","colab_type":"code","colab":{}},"source":["def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length = MAX_LENGTH):\n","  encoder_hidden = encoder.initHidden()\n","  encoder_optimizer.zero_grad()\n","  decoder_optimizer.zero_grad()\n","  input_length = input_tensor.size(0)\n","  target_length = target_tensor.size(0)\n","  encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device = device)\n","  loss = 0\n","  for ei in range(input_length):\n","    encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n","    encoder_outputs[ei] = encoder_output[0,0]\n","  decoder_input = torch.tensor([[SOS_token]], device = devide)\n","  decoder_hidden = encoder_hidden\n","  use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","  if use_teacher_forcing:\n","    for di in range(target_length):\n","      decoder_output, ddecoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n","      loss += criterion(decoder_output, target_tensor[di])\n","      decoder_input = target_tensor[di]\n","  else:\n","    for di in range(target_length):\n","      decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n","      topv, topi = decoder_output.topk(1)\n","      decoder_input = topi.squeeze().detach()\n","\n","      loss += criterion(decoder_output, target_tensor[di])\n","      if decoder_input.item() == EOS_token:\n","        break\n","  loss.backward()\n","\n","  encoder_optimizer.step()\n","  decoder_optimizer.step()\n","\n","  return loss.item() / target_length"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"llFvsmERWb1w","colab_type":"code","colab":{}},"source":["import time\n","import math\n","\n","def asMinutes(s):\n","  m = math.floor(s/60)\n","  s -= m*60\n","  return '%dm %ds' %(m,s)\n","\n","def timeSince(since, percent):\n","  now = time.time()\n","  s = now - since \n","  es = s / (percent)\n","  rs = es -2 \n","  return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n","\n","def trainIters(encoder, decoder, n_iters, print_every = 1000, plot_every = 100 , learning_rate = 0.01):\n","  start = time.time()\n","  plot_losses = []\n","  print_loss_total = 0\n","  plot_loss_total = 0\n","\n","  encoder_optimizer = optim.SGD(encoder.parameters(), lr = learning_rate)\n","  decoder_optimizer = optim.SGD(decoder.parameters(), lr = learning_rate)\n","  training_pairs = [tensorFromPair(random.choice(pairs)) for i in range(n_iters)]\n","  criterion = nn.NLLLoss()\n","  for iter in range(1, n_iters +1):\n","    training_pair = training_pairs[iter - 1]\n","    input_tensor = training_pair[0]\n","    target_tensor = training_pair[1]\n","\n","    loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n","    print_loss_total += loss\n","    plot_loss_total += loss\n","    if iter % print_every == 0:\n","      print_loss_avg = print_loss_total / print_every\n","      print_loss_total = 0\n","      print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters), iter, iter / n_iters * 100, print_loss_avg))\n","    if iter % plot_every == 0:\n","      plot_loss_avg = plot_loss_total / plot_every\n","      plot_losses.append(plot_loss_avg)\n","      plot_loss_total = 0\n","  showPlot(plot_losses)\n","\n","                                   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dFmICmhAZjap","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","plt.switch_backend('agg')\n","import matplotlib.ticker as ticker\n","import numpy as np\n","def showPlot(points):\n","  plt.figure()\n","  fig, ax = plt.subplots()\n","  loc = ticker.MultipleLocater(base = 0.2)\n","  ax.yaxis.set_major_locator(loc)\n","  plt.plot(points)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z2S4jkg9UhL5","colab_type":"code","colab":{}},"source":["def evaluate(encoder, decoder, sentence, max_length = MAX_LENGTH):\n","  with torch.no_grad():\n","    input_tensor = tensorFromSentence(input_lang, sentence)\n","    input_length = input_tensor.size()[0]\n","    encoder_hidden = encoder.initHidden()\n","    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device = device)\n","\n","    for ei in range(input_length):\n","      encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n","      encoder_outputs[ei] += endocer_output[0,0]\n","    decoder_input = torch.tensor([[SOS_token]], device = device)\n","    decoder_hidden = encoder_hidden\n","    decode_words = []\n","    decoder_attentions = torch.zeros(max_length, max_length)\n","    for di in range(max_length):\n","      decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n","      decoder_attentions[di] = decoder_attention.data\n","      topv, topi = decoder_output.data.topk(1)\n","      if topi.item() == EOS_token:\n","        decoded_words.append('<EOS>')\n","        break\n","      else:\n","        decode_words.append(output_lang.indexes2word[topi.item()])\n","      decoder_input = topi.squeeze().detatch()\n","    return dcoded_words, decoder_attentions[:di + 1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zEDCjN4Sc8Vp","colab_type":"code","colab":{}},"source":["def evaluateRandomly(encoder, decoder, n = 10):\n","  for i in range(n):\n","    pair = random.choice(pairs)\n","    print('>', pair[0])\n","    print('=', pair[1])\n","    output_words, attentions = evaluate(encoder, decoder, pair[0])\n","    output_sentence = ' '.join(output_words)\n","    print('<', output_sentence)\n","    print('')\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HCWHpy5-dg0g","colab_type":"code","colab":{}},"source":["hidden_size = 256\n","encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n","attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p = 0.1).to(device)\n","trainIters(encoder1, attn_decoder1, 75000, print_every = 5000)\n","evaluateRandomly(encoder1, attn_decoder1)\n","output_words, attentions = evaluate(encoder1, attn_decoder1, \"je suis trop froid .\")\n","plt.matshow(attentions.numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wlbH8ZcIeUyd","colab_type":"code","colab":{}},"source":["def showAttention(input_sentence,output_words, attentions):\n","  fig = ply.figure()\n","  ax = fig.add_subplot(111)\n","  cax = ax.matshow(attentions.numpy(), cmap= 'bone')\n","  fig.colorbar(cax)\n","  ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation = 90)\n","  ax.set_yticklabels([''] + output_words)\n","\n","  ax.xaxis.set_major_locater(ticker.MultipleLocator(1))\n","  ax.yaxos.set_major_locater(ticker.MultipleLocator(1))\n","\n","  plt.show()\n","  \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mw1YVQZnfTRr","colab_type":"code","colab":{}},"source":["def evaluateAndShowAttention(input_sentence):\n","  output_words, attentions = evaluate(encoder1, attn_decoder1, input_sentence)\n","  print('input = ', input_sentence)\n","  print('output = ', ' '.join(output_words))\n","  showAttention(input_sentence, output_words, attentions)\n","\n","evaluateAndShowAttention(\"elle a ciq ans de moins que moi .\")\n","evaluateAndShowAttention(\"elle est trop petit .\")\n","evaluateAndShowAttention(\"je ne crains pas de mourir.\")\n","evaluateAndShowAttention(\"c est in jeune directeur plein de talent\")\n"],"execution_count":null,"outputs":[]}]}