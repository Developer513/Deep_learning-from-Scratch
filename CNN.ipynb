{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"mount_file_id":"1r5BWKC42iodcGWuMGpHlq1y9fPXtvg4a","authorship_tag":"ABX9TyNP9mUzK94Nrh8M0wbsCoi5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"mcxQi1xyhumE","colab_type":"code","colab":{}},"source":["import torch \n","import torchvision.datasets as dsets\n","import torchvision.transforms as transforms\n","import torch.nn.init"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EBFyrtN6jtAN","colab_type":"code","colab":{}},"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6pjpvsGQj24h","colab_type":"code","colab":{}},"source":["torch.manual_seed(777)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KoPGERkKkTA7","colab_type":"code","colab":{}},"source":["if device == 'cuda':\n","  torch.cuda.manual_seed_all(777)\n","\n","learning_rate = 0.001\n","training_epochs = 15\n","batch_size = 100\n","#배치 크기는 보통 2의 제곱수를 사용합니다. ex) 2, 4, 8, 16, 32, 64... 그 이유는 CPU와 GPU의 메모리가 2의 배수이므로 배치크기가 2의 제곱수일 경우에 데이터 송수신의 효율을 높일 수 있다고 합니다"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2ds3PkBEkmnw","colab_type":"code","colab":{}},"source":["mnist_train = dsets.MNIST(root = 'MNIST_data/',train = True, transform = transforms.ToTensor(),download = True)\n","mnist_test = dsets.MNIST(root = 'MNIST_data/',train = False, transform = transforms.ToTensor(), download = True)\n","data_loader = torch.utils.data.DataLoader(dataset = mnist_train, batch_size = batch_size, shuffle = True, drop_last = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wmZSvgy5lQUD","colab_type":"code","colab":{}},"source":["class CNN(torch.nn.Module):\n","  def __init__(self):\n","    super(CNN, self).__init__()\n","    self.keep_prob = 0.5\n","    # 이미지 처리의 기본적인 용어 높이 : 이미지의 세로방향 픽셀, 너비 : 이미지의 가로바양 픽셀\n","    # 채널 : 이미지의 색 성분이다. 흑백이미지 이면 채널값은 1이다\n","                  # 합성곱 연산 메소드 1채널 짜리를 입력받아 32채널을 결과값을 생성, 커 널    \n","    self.layer1 = torch.nn.Sequential(torch.nn.Conv2d(1, 32, kernel_size = 3, stride = 1, padding = 1),\n","                                     torch.nn.ReLU(),torch.nn.MaxPool2d(kernel_size = 2, stride = 2))\n","                  # 32채널을 입력받아 64 채널의  결과값 생성\n","    self.layer2 = torch.nn.Sequential(torch.nn.Conv2d(32, 64, kernel_size = 3, stride = 1, padding = 1),\n","                                     torch.nn.ReLU(),torch.nn.MaxPool2d(kernel_size = 2, stride = 2))\n","                  # 64채널을 입력받아 128 채널의 결과값 생성\n","    self.layer3 = torch.nn.Sequential(torch.nn.Conv2d(64,128, kernel_size = 3, stride = 1, padding = 1),\n","                                     torch.nn.ReLU(),torch.nn.MaxPool2d(kernel_size = 2, stride=2, padding = 1))\n","    #self.layer3 = torch.nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1)\n","\n","    #self.fc =  torch.nn.Linear(7*7*64, 10, bias = True)\n","    # ??\n","    self.fc1 =  torch.nn.Linear(4*4*128, 625, bias = True) # fully connected 선언 \n","    torch.nn.init.xavier_uniform_(self.fc1.weight)\n","    # fully connected 다음 \n","    self.layer4 = torch.nn.Sequential(\n","            self.fc1, # fulltconnected 실행\n","            torch.nn.ReLU(),\n","            torch.nn.Dropout(p=1 - self.keep_prob))# 오버피팅을 방지하기 위해 학습과정에서 신경망의 일부를 사용하지 않느다\n","            # 0.5 이면 절반만 사용함\n","    # L5 Final FC 625 inputs -> 10 outputs\n","    self.fc2 = torch.nn.Linear(625, 10, bias=True) # 생성된 625개의 채널을 최종 출력(1~10) 만큼 변환\n","    torch.nn.init.xavier_uniform_(self.fc2.weight)\n","  def forward(self, x):\n","    out = self.layer1(x)\n","    out = self.layer2(out)\n","    out = self.layer3(out)\n","    out = out.view(out.size(0), -1)\n","    out = self.layer4(out)\n","    out = self.fc2(out)\n","    return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bIFUA9GQl-6O","colab_type":"code","colab":{}},"source":["model = CNN().to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_ytNqkzlmAd8","colab_type":"code","colab":{}},"source":["criterion = torch.nn.CrossEntropyLoss().to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s8_FDCgnpHMq","colab_type":"code","colab":{}},"source":["optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_1oT5O7GpWir","colab_type":"code","colab":{}},"source":["total_batch = len(data_loader)\n","print('총 배치의 수 : {}' .format(total_batch))\n","for epoch in range(training_epochs):\n","  avg_cost = 0\n","  for X,Y in data_loader:\n","    X = X.to(device)\n","    Y = Y.to(device)\n","\n","    optimizer.zero_grad()\n","    hypothesis = model(X)\n","    cost = criterion(hypothesis, Y)\n","    cost.backward()\n","    optimizer.step()\n","\n","    avg_cost += cost / total_batch\n","print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WPeMqko1ryhm","colab_type":"code","colab":{}},"source":["with torch.no_grad():\n","  X_test = mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float().to(device)\n","  Y_test = mnist_test.test_labels.to(device)\n","\n","  prediction = model(X_test)\n","  correct_prediction = torch.argmax(prediction, 1) == Y_test\n","  accuracy = correct_prediction.float().mean()\n","  print('Accuracy:', accuracy.item())"],"execution_count":null,"outputs":[]}]}